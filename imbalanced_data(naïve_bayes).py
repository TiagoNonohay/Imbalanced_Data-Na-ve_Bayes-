# -*- coding: utf-8 -*-
"""Imbalanced_Data(Naïve_Bayes).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GdiRQeZKdeBUyDHL1-7Sa_aNo7zC9nrF

# Imbalanced Data | Machine Learning

#New Database and training
"""

import random
import pandas as pd
import numpy as np
import seaborn as sns

dataset = pd.read_csv('credit_data.csv')
dataset.shape

# checking NaN or missing values
pd.isna(dataset).sum()

#removing 3 missing values
dataset.dropna(inplace=True)
dataset.shape

dataset.head()

"""# Spliting Database in **label** and **class**"""

#For training and test: split database in "label" and "class"
x = dataset.iloc[:, 1:4].values
y = dataset.iloc[:, 4].values

# x as label
x

# y as class
y

"""# Training and Test"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify = y)

x_train.shape, y_train.shape

x_test.shape, y_test.shape

np.unique(y, return_counts=True)

1714/ len(dataset), 283/len(dataset)

"""value 0= 86% / value 1= 14%
"Let's assume 0 as paying the loan (1714) and 1 as not (283)"
"""

#stratified sample check
np.unique(y_train, return_counts=True)

np.unique(y_test, return_counts=True)

57/len(y_test)

"""# Naïve Bayes Classification"""

from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(x_train, y_train)

prevision= model.predict(x_test)
prevision

y_test

from sklearn.metrics import accuracy_score

# testing accuracy from predective model (Result = 92%)
accuracy_score(prevision, y_test)

from sklearn.metrics import confusion_matrix
confusion_matrix(prevision, y_test)

sns.heatmap(confusion_matrix(y_test, prevision), annot=True)

"""# Accuracy test"""

#testing accuracy from all model
(336+31) / (336+26+7+31)

# pay the loan (sucess %)
336/(336+26)

# not pay the loan (sucess %)
31/(31+7)

"""At this point, we have an 18%  error, which means: the system allows loans to sensitive clients (bad score).

# Undersampling - Tomek Links
"""

x.shape, y.shape

from imblearn.under_sampling import TomekLinks

# Resampling only the majority class
undersampling= TomekLinks(sampling_strategy='majority')
x_under, y_under = undersampling.fit_resample(x, y)
x_under.shape, y_under.shape

"""Undersampling technique removed 100 logs from database."""

# original database
np.unique(y, return_counts=True)

# new database after undersampling
np.unique(y_under, return_counts=True)

# test fixed in 20% | stratified sample
x_train_under, x_test_under, y_train_under, y_test_under = train_test_split(x_under, y_under, test_size = 0.2, stratify = y_under)

x_train_under.shape, x_test_under.shape

model = GaussianNB()
model.fit(x_train_under, y_train_under)
predictions = model.predict(x_test_under)

accuracy_score(predictions, y_test_under)

matrix_under = confusion_matrix(predictions, y_test_under)
matrix_under

317/(317+22)

35/(35+6)

"""Accuracy has been increased in both cases, but it needs to be more sufficient to prevent money loss.

--------------

# Oversampling - SMOTE
"""

from imblearn.over_sampling import SMOTE

sm= SMOTE(sampling_strategy='minority')
x_smote, y_smote = sm.fit_resample(x, y)
x_smote.shape, y_smote.shape

# originl logs
np.unique(y, return_counts=True)

# balanced logs
np.unique(y_smote, return_counts=True)

x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(x_smote, y_smote, test_size = 0.2, stratify = y_smote)

x_train_smote.shape, x_test_smote.shape

model = GaussianNB()
model.fit(x_train_smote, y_train_smote)
predictions = model.predict(x_test_smote)
accuracy_score(predictions, y_test_smote)

cm_smote = confusion_matrix(predictions, y_test_smote)
cm_smote

291/(291+16)

327/(327+52)

"""After oversampling, we have a 14% error rate, which means that accuracy has increased by 4%. For example, in 1000 clients, this represents 40 clients."""